experiment: stage2a_prompt_vitb16_11k_dorsal_r
dataset: 11k
aspect: dorsal_r
model: vitb16
variant: clipid

n_ctx: 8                        # Number of learnable context tokens
ctx_init: null                 # Use null for random, or "hand" / "identity" etc.
prompt_template: "A photo of a {} hand."  # Prompt format
freeze_text_encoder: true  # true = CoOp-style | false = full-tuning
early_stop_prompt_norm: 1.2e-3

batch_size: 32
epochs: 30
lr: 0.0001

output_dir: train_logs/
save_dir: saved_models/

loss_list: ["supcon"]   # CLIP-ReID uses supcon_loss for prompt tuning



