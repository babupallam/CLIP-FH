Implementation of experiments/stage1_train_classifier_frozen_text/train_stage1_frozen_text.py

========================

**main()**  
   - **take configuration parameters** using `config.get("", default value)`  
     - These parameters include things like `epochs`, `lr`, `batch_size`, `early_stop_patience`, etc.  
     - They control how many epochs we train, what learning rate we use, etc.  
   - **bulding filename** `build_filename()`  
     - This function creates a systematic filename for saving logs and model checkpoints (e.g., with epoch number and timestamp).  
   - **device selection**  
     - Check if a GPU is available (`cuda`) or fall back to CPU. Ensures faster computation if GPU is present.  
   - **train, val, classes loader** `get_train_val_loaders(config)`  
     - **from the dataset**: Loads images from folder structures (train, query0, gallery0).  
     - **transform() - used for** resizing images to 224×224, converting them to PyTorch tensors, etc. Ensures uniform input size and format.  
   - **build model()** - which creates CLIP model and classifier  
     - **why those are used for**  
       - **clip model**: Provides the frozen text encoder and the fine-tunable image encoder to extract powerful image embeddings.  
       - **classifier**: A small linear head (e.g. `nn.Linear(embed_dim, num_classes)`) that maps the CLIP image embeddings to class logits.  
     - **assign clip_model** = using the model variant, using `clip.load()`  
       - This loads the chosen CLIP architecture (ViT-B/32, RN50, etc.) so we can use its `visual` component.  
     - **classifer** is created using a linear layer of `nn` with `image_embeddim x number_of_classes` dimension using `nn.Linear()`.  
       - The output has size `[batch_size, num_classes]`, which fits the CrossEntropy classification setup.  
   - **assign trainer class** = `FinetuneTrainerStage1(clip_model, classifier, train_loader, val_loader, config, device)` from `engine/train_classifier_stage1.py`  
     - This class encapsulates the training logic so we keep our code structured.  
   - **exicute train() function** in trainer class  
     - **in the constructor `__init__()`** of `FinetuneTrainerStage1` class, it does  
       - **assigning loaders**: Receives `train_loader` and `val_loader`  
       - **assigning parameters**: Reads learning rate, epochs, and other hyperparameters from `config`.  
       - **assign loss functions**  
         - `crossEntropyLoss()`: For classification accuracy. Helps the classifier distinguish classes.  
         - `tripletLoss()`: Encourages embeddings of the same class to be closer in feature space, and different classes to be farther apart.  
       - **early stopping parameter** `self.early_stop_patience`  
         - If `Rank-1` validation accuracy stops improving after this many epochs, training stops early.  
       - **assign optimizer = Adam optimizer**  
         - the parameter changes in this is  
           - `clip_model.visual.parameters(), "lr": self.lr}`  
             - **why?** Because we want to fine-tune the image encoder at the main learning rate.  
           - `classifier.parameters(), "lr": self.lr * 0.1}`  
             - **why?** Because the classifier is simpler and may need a smaller learning rate to avoid overfitting or to stabilize training.  
       - **setting the log directory**  
         - Where logs will be stored (such as training loss, rank1, rank5, rank10, mAP, and debug info like `logits std`).  
       - **in train()**  
         - **set clip in training mode** `clip_model.train()`  
           - Ensures dropout, batch norm, etc., are active in training mode for the image encoder.  
         - **for each epoch**  
           - **for each batch of data**  
             - **images and labels**: Tensors from `DataLoader`  
             - **resetting optimizer values**: `optimizer.zero_grad()`  
             - **use clip to encode images**: `features = clip_model.encode_image(images)`  
               - We’re freezing the text side, but the image encoder learns.  
             - **output is the classifier output** on extracted features  
               - i.e., `outputs = classifier(features)`  
             - **calculate loss**  
               - **calculate crossentroppy loss on outputs, labels**  
                 - E.g. `ce = ce_loss(outputs, labels)`  
               - **calculate triplet loss on features, labels**  
                 - E.g. `tri = triplet_loss(features, labels)`  
               - **calculate sum of losses**: `loss = ce + tri`  
                 - This ensures classification correctness and feature separation.  
             - **back propogation of the loss**  
               - `loss.backward()` accumulates gradients.  
             - **Clip gradients** to avoid exploding gradients  
               - e.g. `clip_grad_norm_` helps keep training stable.  
             - **update the model parameters** using `optmizer.step()`  
               - Applies the gradients to fine-tune `clip_model.visual` and the classifier.  
             - Additionally, the code logs parameters like **Rank-1** / **Rank-5** / **Rank-10** accuracy, **mAP** for ReID evaluation, and **logits std** for diagnostic purposes.  
               - **Rank-1**: How many times the correct label is in the top-1 prediction  
               - **Rank-5**: How many times the correct label is in the top-5 predictions  
               - **Rank-10**: How many times the correct label is in the top-10 predictions  
               - **mAP (mean Average Precision)**: Measures overall retrieval performance, especially in ReID tasks  
               - **logits std**: Debug measure to see if outputs are saturating or exploding.  

